# ğŸ’£ MineRL-LLM â€” Reinforcement Learning Powered Minesweeper Agent

> ğŸ† Developed during the AMD AI Hackathon at IIT Delhi  
> ğŸ”¥ Fine-tuned GPT-OSS-20B using LoRA + GRPO on AMD GPUs (ROCm)

---

## ğŸš€ Overview

MineRL-LLM is a Reinforcement Learning-based Large Language Model trained to play Minesweeper.

The model:

- ğŸ“¥ Takes a JSON board state as input  
- ğŸ“¤ Outputs a structured JSON action (`reveal` or `flag`)  
- ğŸ¯ Learns optimal gameplay through GRPO (Group Relative Policy Optimization)  
- âš¡ Trained using Unsloth framework for accelerated LoRA finetuning  

This project demonstrates how LLMs can be adapted to structured decision-making environments using RL.

---

## ğŸ§  Training Approach

| Component | Details |
|------------|----------|
| Base Model | GPT-OSS-20B (BF16) |
| Finetuning | LoRA (Rank 16) |
| RL Method | GRPO |
| Framework | Unsloth |
| Hardware | AMD GPU (ROCm) |
| Dataset | 1000 dynamically generated Minesweeper states |

---

## ğŸ— Model Architecture

- Low-Rank Adaptation (LoRA) applied to:
  - `q_proj`
  - `k_proj`
  - `v_proj`
  - `o_proj`
  - `gate_proj`
  - `up_proj`
  - `down_proj`

- Only ~0.04% parameters trained (~8M parameters)
- Base model weights remain frozen

---

## ğŸ® Game Interface

### Input Format

```json
{
  "board": [[".", ".", "."], ["1", ".", "."], ["0", "1", "."]],
  "rows": 3,
  "cols": 3,
  "mines": 1,
  "flags_placed": 0,
  "cells_revealed": 2
}
